{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaU4BeFCHuEfQtEalkBXYu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinachengece/ENEE739F_HW2/blob/main/ENEE739F_HW2_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RggJnfcmWio",
        "outputId": "39b8ce9b-d8cb-4d14-b858-719315e0f278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            " The future of machine learning is in the hands of the next generation of machine learning researchers.\n",
            "\n",
            "Machine learning is a new field\n"
          ]
        }
      ],
      "source": [
        "# Problem 2 Part (a) to (c)\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# load small GPT-2\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "prompt = \"The future of machine learning is\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "# Greedy generation: 20 new tokens\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=20,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated text:\\n\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2 Part (d) to (e): Explainability of GPT-2\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits[0, -1, :]\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "top_ids = torch.argsort(probs, descending=True)\n",
        "\n",
        "def is_visible_token(tok_id):\n",
        "    text = tokenizer.decode([tok_id])\n",
        "    return text.strip() != \"\"\n",
        "\n",
        "for tok_id in top_ids:\n",
        "    tok_id = tok_id.item()\n",
        "    if is_visible_token(tok_id):\n",
        "        target_token_id = tok_id\n",
        "        target_word = tokenizer.decode([tok_id])\n",
        "        break\n",
        "\n",
        "print(\"Target token ID:\", target_token_id)\n",
        "print(\"Target word:\", repr(target_word))\n",
        "\n",
        "baseline_ids = torch.full_like(input_ids, fill_value=tokenizer.eos_token_id)\n",
        "print(\"Baseline:\", baseline_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQXTaHfbxQOT",
        "outputId": "340ba325-7bb3-4f85-8fb3-30a884aa3db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target token ID: 287\n",
            "Target word: ' in'\n",
            "Baseline: tensor([[50256, 50256, 50256, 50256, 50256, 50256]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2 Part (f) to (h)\n",
        "with torch.no_grad():\n",
        "    input_embeds = model.transformer.wte(input_ids)\n",
        "    baseline_embeds = model.transformer.wte(baseline_ids)\n",
        "\n",
        "print(\"Embedding shape:\", input_embeds.shape)\n",
        "\n",
        "# 2) Define forward function for IG\n",
        "def forward_on_embeddings(inputs_embeds: torch.Tensor):\n",
        "\n",
        "    outputs = model(inputs_embeds=inputs_embeds)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "    # Return the logit for the target token\n",
        "    target_logit = next_token_logits[:, target_token_id]  # [1]\n",
        "    return target_logit\n",
        "\n",
        "\n",
        "# 3) Initialize IG\n",
        "ig = IntegratedGradients(forward_on_embeddings)\n",
        "\n",
        "# 4) Compute attributions\n",
        "attributions, delta = ig.attribute(\n",
        "    inputs=input_embeds,\n",
        "    baselines=baseline_embeds,\n",
        "    n_steps=50,\n",
        "    return_convergence_delta=True\n",
        ")\n",
        "\n",
        "print(\"Attributions shape:\", attributions.shape)\n",
        "print(\"Convergence delta:\", float(delta))\n",
        "\n",
        "token_importances = attributions.sum(dim=-1).squeeze(0)\n",
        "token_importances = token_importances.detach().cpu()\n",
        "input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "for tok, score in zip(input_tokens, token_importances):\n",
        "    print(f\"{tok:>10s}  ->  {float(score): .6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH5QWZEazCsL",
        "outputId": "c27588ac-ace9-4111-941d-8066ed665033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([1, 6, 768])\n",
            "Attributions shape: torch.Size([1, 6, 768])\n",
            "Convergence delta: 5.002692222595215\n",
            "       The  ->  -13.728738\n",
            "   Ġfuture  ->  -23.914446\n",
            "       Ġof  ->  -16.973946\n",
            "  Ġmachine  ->   1.334663\n",
            " Ġlearning  ->   14.426741\n",
            "       Ġis  ->   27.997829\n"
          ]
        }
      ]
    }
  ]
}